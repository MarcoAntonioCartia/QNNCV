{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nQuantum state learning\n======================\n\nThis demonstration works through the process used to produce the state\npreparation results presented in `\"Machine learning method for state\npreparation and gate synthesis on photonic quantum\ncomputers\" <https://arxiv.org/abs/1807.10781>`__.\n\nThis tutorial uses the TensorFlow backend of Strawberry Fields, giving us access\nto a number of\nadditional functionalities including: GPU integration, automatic gradient\ncomputation, built-in optimization algorithms, and other machine\nlearning tools.\n\nVariational quantum circuits\n----------------------------\n\nA key element of machine learning is optimization. We can use\nTensorFlow's automatic differentiation tools to optimize the parameters\nof variational quantum circuits constructed using Strawberry Fields. In\nthis approach, we fix a circuit architecture where the states, gates,\nand/or measurements may have learnable parameters $\\vec{\\theta}$\nassociated with them. We then define a loss function based on the output\nstate of this circuit. In this case, we define a loss function such that\nthe fidelity of the output state of the variational circuit is maximized\nwith respect to some target state.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For more details on the TensorFlow backend in Strawberry Fields, please see\n    `machine_learning_tutorial`.</p></div>\n\n\nFor arbitrary state preparation using optimization, we need to make use\nof a quantum circuit with a layer structure that is **universal** - that\nis, by 'stacking' the layers, we can guarantee that we can produce *any*\nCV state with at-most polynomial overhead. Therefore, the architecture\nwe choose must consist of layers with each layer containing\nparameterized Gaussian *and* non-Gaussian gates. **The non-Gaussian\ngates provide both the nonlinearity and the universality of the model.**\nTo this end, we employ the CV quantum neural network architecture as described in\n`Killoran et al. <https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.1.033063>`__:\n\n.. figure:: https://i.imgur.com/NEsaVIX.png\n   :alt: layer\n\nHere,\n\n-  $\\mathcal{U}_i(\\theta_i,\\phi_i)$ is an N-mode linear optical\n   interferometer composed of two-mode beamsplitters\n   $BS(\\theta,\\phi)$ and single-mode rotation gates\n   $R(\\phi)=e^{i\\phi\\hat{n}}$,\n\n-  $\\mathcal{D}(\\alpha_i)$ are single mode displacements in the\n   phase space by complex value $\\alpha_i$,\n\n-  $\\mathcal{S}(r_i, \\phi_i)$ are single mode squeezing operations\n   of magnitude $r_i$ and phase $\\phi_i$, and\n\n-  $\\Phi(\\lambda_i)$ is a single mode non-Gaussian operation, in\n   this case chosen to be the Kerr interaction\n   $\\mathcal{K}(\\kappa_i)=e^{i\\kappa_i\\hat{n}^2}$ of strength\n   $\\kappa_i$.\n\n\nHyperparameters\n---------------\n\nFirst, we must define the **hyperparameters** of our layer structure:\n\n-  ``cutoff``: the simulation Fock space truncation we will use in the\n   optimization. The TensorFlow backend will perform numerical\n   operations in this truncated Fock space when performing the\n   optimization.\n\n-  ``depth``: The number of layers in our variational quantum\n   circuit. As a general rule, increasing the number of layers (and\n   thus, the number of parameters we are optimizing over) increases the\n   optimizer's chance of finding a reasonable local minimum in the\n   optimization landscape.\n\n-  ``reps``: the number of steps in the optimization routine performing\n   gradient descent\n\nSome other optional hyperparameters include:\n\n-  The standard deviation of initial parameters. Note that we make a\n   distinction between the standard deviation of *passive* parameters\n   (those that preserve photon number when changed, such as phase\n   parameters), and *active* parameters (those that introduce or remove\n   energy from the system when changed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport strawberryfields as sf\nfrom strawberryfields.ops import *\nfrom strawberryfields.utils import operation\n\n# Cutoff dimension\ncutoff = 9\n\n# Number of layers\ndepth = 15\n\n# Number of steps in optimization routine performing gradient descent\nreps = 200\n\n# Learning rate\nlr = 0.05\n\n# Standard deviation of initial parameters\npassive_sd = 0.1\nactive_sd = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The layer parameters $\\vec{\\theta}$\n-----------------------------------------\n\nWe use TensorFlow to create the variables corresponding to the gate\nparameters. Note that we focus on a single mode circuit where\neach variable has shape ``(depth,)``, with each\nindividual element representing the gate parameter in layer $i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\n# set the random seed\ntf.random.set_seed(42)\n\n# squeeze gate\nsq_r = tf.random.normal(shape=[depth], stddev=active_sd)\nsq_phi = tf.random.normal(shape=[depth], stddev=passive_sd)\n\n# displacement gate\nd_r = tf.random.normal(shape=[depth], stddev=active_sd)\nd_phi = tf.random.normal(shape=[depth], stddev=passive_sd)\n\n# rotation gates\nr1 = tf.random.normal(shape=[depth], stddev=passive_sd)\nr2 = tf.random.normal(shape=[depth], stddev=passive_sd)\n\n# kerr gate\nkappa = tf.random.normal(shape=[depth], stddev=active_sd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience, we store the TensorFlow variables representing the\nweights as a tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = tf.convert_to_tensor([r1, sq_r, sq_phi, r2, d_r, d_phi, kappa])\nweights = tf.Variable(tf.transpose(weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have a depth of 15 (so 15 layers), and each layer takes\n7 different types of parameters, the final shape of our weights\narray should be $\\text{depth}\\times 7$ or ``(15, 7)``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(weights.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constructing the circuit\n------------------------\n\nWe can now construct the corresponding\nsingle-mode Strawberry Fields program:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Single-mode Strawberry Fields program\nprog = sf.Program(1)\n\n# Create the 7 Strawberry Fields free parameters for each layer\nsf_params = []\nnames = [\"r1\", \"sq_r\", \"sq_phi\", \"r2\", \"d_r\", \"d_phi\", \"kappa\"]\n\nfor i in range(depth):\n    # For the ith layer, generate parameter names \"r1_i\", \"sq_r_i\", etc.\n    sf_params_names = [\"{}_{}\".format(n, i) for n in names]\n    # Create the parameters, and append them to our list ``sf_params``.\n    sf_params.append(prog.params(*sf_params_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``sf_params`` is now a nested list of shape ``(depth, 7)``, matching\nthe shape of ``weights``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sf_params = np.array(sf_params)\nprint(sf_params.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can create a function to define the $i$\\ th layer, acting on qumode ``q``. We add\nthe :class:`~strawberryfields.utils.operation` decorator so that the layer can be used as a single\noperation when constructing our circuit within the usual Strawberry Fields Program context\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# layer architecture\n@operation(1)\ndef layer(i, q):\n    Rgate(sf_params[i][0]) | q\n    Sgate(sf_params[i][1], sf_params[i][2]) | q\n    Rgate(sf_params[i][3]) | q\n    Dgate(sf_params[i][4], sf_params[i][5]) | q\n    Kgate(sf_params[i][6]) | q\n    return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have defined our gate parameters and our layer structure, we\ncan construct our variational quantum circuit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply circuit of layers with corresponding depth\nwith prog.context as q:\n    for k in range(depth):\n        layer(k) | q[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the optimization\n---------------------------\n\n$\\newcommand{ket}[1]{\\left|#1\\right\\rangle}$ With the Strawberry\nFields TensorFlow backend calculating the resulting state of the circuit\nsymbolically, we can use TensorFlow to optimize the gate parameters to\nminimize the cost function we specify. With state learning, the measure\nof distance between two quantum states is given by the fidelity of the\noutput state $\\ket{\\psi}$ with some target state\n$\\ket{\\psi_t}$. This is defined as the overlap between the two\nstates:\n\n\\begin{align}F = \\left|\\left\\langle{\\psi}\\mid{\\psi_t}\\right\\rangle\\right|^2\\end{align}\n\nwhere the output state can be written\n$\\ket{\\psi}=U(\\vec{\\theta})\\ket{\\psi_0}$, with\n$U(\\vec{\\theta})$ the unitary operation applied by the variational\nquantum circuit, and $\\ket{\\psi_0}=\\ket{0}$ the initial state.\n\nLet's first instantiate the TensorFlow backend, making sure to pass\nthe Fock basis truncation cutoff.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": cutoff})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define the target state as the single photon state\n$\\ket{\\psi_t}=\\ket{1}$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ntarget_state = np.zeros([cutoff])\ntarget_state[1] = 1\nprint(target_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this target state, we calculate the fidelity with the state\nexiting the variational circuit. We must use TensorFlow functions to\nmanipulate this data, as well as a ``GradientTape`` to keep track of the\ncorresponding gradients!\n\nWe choose the following cost function:\n\n\\begin{align}C(\\vec{\\theta}) = \\left| \\langle \\psi_t \\mid U(\\vec{\\theta})\\mid 0\\rangle - 1\\right|\\end{align}\n\nBy minimizing this cost function, the variational quantum circuit will\nprepare a state with high fidelity to the target state.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(weights):\n    # Create a dictionary mapping from the names of the Strawberry Fields\n    # free parameters to the TensorFlow weight values.\n    mapping = {p.name: w for p, w in zip(sf_params.flatten(), tf.reshape(weights, [-1]))}\n\n    # Run engine\n    state = eng.run(prog, args=mapping).state\n\n    # Extract the statevector\n    ket = state.ket()\n\n    # Compute the fidelity between the output statevector\n    # and the target state.\n    fidelity = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state)) ** 2\n\n    # Objective function to minimize\n    cost = tf.abs(tf.reduce_sum(tf.math.conj(ket) * target_state) - 1)\n    return cost, fidelity, ket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the cost function is defined, we can define and run the\noptimization. Below, we choose the Adam\noptimizer that is built into TensorFlow:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then loop over all repetitions, storing the best predicted fidelity\nvalue.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fid_progress = []\nbest_fid = 0\n\nfor i in range(reps):\n    # reset the engine if it has already been executed\n    if eng.run_progs:\n        eng.reset()\n\n    with tf.GradientTape() as tape:\n        loss, fid, ket = cost(weights)\n\n    # Stores fidelity at each step\n    fid_progress.append(fid.numpy())\n\n    if fid > best_fid:\n        # store the new best fidelity and best state\n        best_fid = fid.numpy()\n        learnt_state = ket.numpy()\n\n    # one repetition of the optimization\n    gradients = tape.gradient(loss, weights)\n    opt.apply_gradients(zip([gradients], [weights]))\n\n    # Prints progress at every rep\n    if i % 1 == 0:\n        print(\"Rep: {} Cost: {:.4f} Fidelity: {:.4f}\".format(i, loss, fid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results and visualisation\n-------------------------\n\nPlotting the fidelity vs.\u00a0optimization step:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"font.sans-serif\"] = [\"Computer Modern Roman\"]\nplt.style.use(\"default\")\n\nplt.plot(fid_progress)\nplt.ylabel(\"Fidelity\")\nplt.xlabel(\"Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the following function to plot the Wigner function of our\ntarget and learnt state:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef wigner(rho):\n    \"\"\"This code is a modified version of the 'iterative' method\n    of the wigner function provided in QuTiP, which is released\n    under the BSD license, with the following copyright notice:\n\n    Copyright (C) 2011 and later, P.D. Nation, J.R. Johansson,\n    A.J.G. Pitchford, C. Granade, and A.L. Grimsmo.\n\n    All rights reserved.\"\"\"\n    import copy\n\n    # Domain parameter for Wigner function plots\n    l = 5.0\n    cutoff = rho.shape[0]\n\n    # Creates 2D grid for Wigner function plots\n    x = np.linspace(-l, l, 100)\n    p = np.linspace(-l, l, 100)\n\n    Q, P = np.meshgrid(x, p)\n    A = (Q + P * 1.0j) / (2 * np.sqrt(2 / 2))\n\n    Wlist = np.array([np.zeros(np.shape(A), dtype=complex) for k in range(cutoff)])\n\n    # Wigner function for |0><0|\n    Wlist[0] = np.exp(-2.0 * np.abs(A) ** 2) / np.pi\n\n    # W = rho(0,0)W(|0><0|)\n    W = np.real(rho[0, 0]) * np.real(Wlist[0])\n\n    for n in range(1, cutoff):\n        Wlist[n] = (2.0 * A * Wlist[n - 1]) / np.sqrt(n)\n        W += 2 * np.real(rho[0, n] * Wlist[n])\n\n    for m in range(1, cutoff):\n        temp = copy.copy(Wlist[m])\n        # Wlist[m] = Wigner function for |m><m|\n        Wlist[m] = (2 * np.conj(A) * temp - np.sqrt(m) * Wlist[m - 1]) / np.sqrt(m)\n\n        # W += rho(m,m)W(|m><m|)\n        W += np.real(rho[m, m] * Wlist[m])\n\n        for n in range(m + 1, cutoff):\n            temp2 = (2 * A * Wlist[n - 1] - np.sqrt(m) * temp) / np.sqrt(n)\n            temp = copy.copy(Wlist[n])\n            # Wlist[n] = Wigner function for |m><n|\n            Wlist[n] = temp2\n\n            # W += rho(m,n)W(|m><n|) + rho(n,m)W(|n><m|)\n            W += 2 * np.real(rho[m, n] * Wlist[n])\n\n    return Q, P, W / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing the density matrices\n$\\rho = \\left|\\psi\\right\\rangle \\left\\langle\\psi\\right|$ of the\ntarget and learnt state,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rho_target = np.outer(target_state, target_state.conj())\nrho_learnt = np.outer(learnt_state, learnt_state.conj())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the Wigner function of the target state:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nX, P, W = wigner(rho_target)\nax.plot_surface(X, P, W, cmap=\"RdYlGn\", lw=0.5, rstride=1, cstride=1)\nax.contour(X, P, W, 10, cmap=\"RdYlGn\", linestyles=\"solid\", offset=-0.17)\nax.set_axis_off()\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the Wigner function of the learnt state:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nX, P, W = wigner(rho_learnt)\nax.plot_surface(X, P, W, cmap=\"RdYlGn\", lw=0.5, rstride=1, cstride=1)\nax.contour(X, P, W, 10, cmap=\"RdYlGn\", linestyles=\"solid\", offset=-0.17)\nax.set_axis_off()\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n1. Juan Miguel Arrazola, Thomas R. Bromley, Josh Izaac, Casey R. Myers,\n   Kamil Br\u00e1dler, and Nathan Killoran. Machine learning method for state\n   preparation and gate synthesis on photonic quantum computers. `Quantum\n   Science and Technology, 4\n   024004 <https://iopscience.iop.org/article/10.1088/2058-9565/aaf59e>`__,\n   (2019).\n\n2. Nathan Killoran, Thomas R. Bromley, Juan Miguel Arrazola, Maria Schuld,\n   Nicolas Quesada, and Seth Lloyd. Continuous-variable quantum neural networks.\n   `Physical Review Research, 1(3), 033063.\n   <https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.1.033063>`__,\n   (2019).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}