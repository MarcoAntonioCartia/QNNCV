"""
Pure Quantum GAN Complete Training System

Demonstrates the full pure quantum architecture:
1. Pure quantum circuits with individual gate parameters
2. Transformation matrices A and T for encoding/decoding
3. Raw measurement extraction (no statistical processing)
4. Quantum Wasserstein loss on measurement space
5. Complete gradient flow preservation

Architecture:
Generator: latent ‚Üí T ‚Üí quantum_encoding ‚Üí PURE_CIRCUIT ‚Üí measurements ‚Üí A‚Åª¬π ‚Üí data
Discriminator: data ‚Üí A ‚Üí quantum_encoding ‚Üí PURE_CIRCUIT ‚Üí measurements ‚Üí classification
Loss: Wasserstein distance on raw measurement space
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suppress TensorFlow warnings
tf.get_logger().setLevel('ERROR')


class PureQuantumGANTrainer:
    """
    Complete trainer for pure quantum GAN with individual gate parameters.
    """
    
    def __init__(self, config=None):
        """
        Initialize pure quantum GAN trainer.
        
        Args:
            config (dict): Training configuration
        """
        # Default configuration
        self.config = config or {
            'latent_dim': 6,
            'output_dim': 2,
            'g_modes': 4,
            'd_modes': 2,
            'layers': 2,
            'cutoff_dim': 6,
            'g_lr': 0.001,
            'd_lr': 0.001,
            'batch_size': 8,
            'epochs': 10,
            'lambda_gp': 10.0,
            'lambda_transform': 1.0,
            'lambda_quantum': 0.5
        }
        
        logger.info(f"üöÄ Initializing Pure Quantum GAN Trainer")
        logger.info(f"üìä Configuration: {self.config}")
        
        # Initialize models
        self._init_models()
        
        # Initialize loss function
        self._init_loss_function()
        
        # Initialize optimizers
        self._init_optimizers()
        
        # Training history
        self.history = {
            'g_loss': [],
            'd_loss': [],
            'w_distance': [],
            'gradient_penalty': [],
            'quantum_regularization': []
        }
    
    def _init_models(self):
        """Initialize generator and discriminator."""
        # Import model classes (in practice, these would be proper imports)
        # For this demo, we'll create placeholder classes
        
        logger.info("üîß Initializing pure quantum models...")
        
        # Generator
        self.generator = self._create_mock_generator()
        
        # Discriminator  
        self.discriminator = self._create_mock_discriminator()
        
        logger.info(f"‚úÖ Models initialized:")
        logger.info(f"   Generator variables: {len(self.generator.trainable_variables)}")
        logger.info(f"   Discriminator variables: {len(self.discriminator.trainable_variables)}")
    
    def _create_mock_generator(self):
        """Create mock generator for demonstration."""
        class MockPureQuantumGenerator:
            def __init__(self, config):
                self.latent_dim = config['latent_dim']
                self.output_dim = config['output_dim']
                self.measurement_dim = config['g_modes'] * 3
                
                # Mock individual quantum gate parameters
                self.quantum_params = []
                for i in range(50):  # 50 individual gate parameters
                    param = tf.Variable(tf.random.normal([]), name=f'quantum_gate_{i}')
                    self.quantum_params.append(param)
                
                # Transformation matrices
                self.T_matrix = tf.Variable(
                    tf.random.orthogonal([self.latent_dim, self.measurement_dim]) * 0.5,
                    name="T_matrix"
                )
                self.A_matrix = tf.Variable(
                    tf.random.normal([self.measurement_dim, self.output_dim], stddev=0.1),
                    name="A_matrix"
                )
                
            @property
            def trainable_variables(self):
                return self.quantum_params + [self.T_matrix, self.A_matrix]
            
            def generate(self, z):
                # Mock generation: T ‚Üí quantum simulation ‚Üí A‚Åª¬π
                quantum_encoding = tf.matmul(z, self.T_matrix)
                
                # Mock quantum circuit execution with individual parameters
                quantum_influence = tf.reduce_sum([p * 0.01 for p in self.quantum_params])
                mock_measurements = quantum_encoding + quantum_influence
                
                # Transform to output
                output = tf.matmul(mock_measurements, self.A_matrix)
                return tf.nn.tanh(output) * 2.0
            
            def get_raw_measurements(self, z):
                quantum_encoding = tf.matmul(z, self.T_matrix)
                quantum_influence = tf.reduce_sum([p * 0.01 for p in self.quantum_params])
                return quantum_encoding + quantum_influence
            
            def compute_transformation_regularization(self):
                A_reg = tf.reduce_mean(tf.square(self.A_matrix))
                T_reg = tf.reduce_mean(tf.square(self.T_matrix))
                return (A_reg + T_reg) * 0.01
        
        return MockPureQuantumGenerator(self.config)
    
    def _create_mock_discriminator(self):
        """Create mock discriminator for demonstration."""
        class MockPureQuantumDiscriminator:
            def __init__(self, config):
                self.input_dim = config['output_dim']
                self.measurement_dim = config['d_modes'] * 3
                
                # Mock individual quantum gate parameters
                self.quantum_params = []
                for i in range(25):  # 25 individual gate parameters
                    param = tf.Variable(tf.random.normal([]), name=f'disc_quantum_gate_{i}')
                    self.quantum_params.append(param)
                
                # Transformation matrix
                self.A_matrix = tf.Variable(
                    tf.random.normal([self.input_dim, self.measurement_dim], stddev=0.1),
                    name="disc_A_matrix"
                )
                
                # Classification network
                self.classifier = tf.keras.Sequential([
                    tf.keras.layers.Dense(8, activation='relu'),
                    tf.keras.layers.Dense(1, activation='sigmoid')
                ])
                
                # Build classifier
                dummy_input = tf.zeros((1, self.measurement_dim))
                _ = self.classifier(dummy_input)
                
            @property
            def trainable_variables(self):
                return (self.quantum_params + [self.A_matrix] + 
                       self.classifier.trainable_variables)
            
            def discriminate(self, x):
                quantum_encoding = tf.matmul(x, self.A_matrix)
                quantum_influence = tf.reduce_sum([p * 0.01 for p in self.quantum_params])
                mock_measurements = quantum_encoding + quantum_influence
                return self.classifier(mock_measurements)
            
            def get_raw_measurements(self, x):
                quantum_encoding = tf.matmul(x, self.A_matrix)
                quantum_influence = tf.reduce_sum([p * 0.01 for p in self.quantum_params])
                return quantum_encoding + quantum_influence
            
            def classify_measurements(self, measurements):
                return self.classifier(measurements)
            
            def compute_transformation_regularization(self):
                return tf.reduce_mean(tf.square(self.A_matrix)) * 0.01
        
        return MockPureQuantumDiscriminator(self.config)
    
    def _init_loss_function(self):
        """Initialize quantum Wasserstein loss function."""
        # Import loss class (in practice, this would be a proper import)
        from quantum_raw_measurement_loss import QuantumRawMeasurementWassersteinLoss
        
        self.loss_fn = QuantumRawMeasurementWassersteinLoss(
            lambda_gp=self.config['lambda_gp'],
            lambda_transform=self.config['lambda_transform'],
            lambda_quantum=self.config['lambda_quantum']
        )
        
        logger.info("‚úÖ Quantum Wasserstein loss initialized")
    
    def _init_optimizers(self):
        """Initialize optimizers."""
        self.g_optimizer = tf.keras.optimizers.Adam(self.config['g_lr'], beta_1=0.5)
        self.d_optimizer = tf.keras.optimizers.Adam(self.config['d_lr'], beta_1=0.5)
        
        logger.info("‚úÖ Optimizers initialized")
    
    def create_bimodal_dataset(self, n_samples=200):
        """Create bimodal dataset for testing."""
        # First mode
        mode1 = tf.random.normal([n_samples//2, 2], mean=[-1.5, -1.5], stddev=0.3)
        
        # Second mode
        mode2 = tf.random.normal([n_samples//2, 2], mean=[1.5, 1.5], stddev=0.3)
        
        # Combine
        data = tf.concat([mode1, mode2], axis=0)
        
        # Shuffle
        indices = tf.random.shuffle(tf.range(n_samples))
        data = tf.gather(data, indices)
        
        return data
    
    def train_step(self, real_batch):
        """
        Single training step with pure quantum architecture.
        
        Args:
            real_batch: Batch of real data
            
        Returns:
            dict: Training metrics
        """
        batch_size = tf.shape(real_batch)[0]
        
        # DISCRIMINATOR TRAINING
        with tf.GradientTape() as d_tape:
            # Generate fake data
            z = tf.random.normal([batch_size, self.config['latent_dim']])
            fake_data = self.generator.generate(z)
            
            # Get raw measurements
            real_measurements = self.discriminator.get_raw_measurements(real_batch)
            fake_measurements = self.discriminator.get_raw_measurements(fake_data)
            
            # Wasserstein distance on raw measurements
            real_scores = self.discriminator.classify_measurements(real_measurements)
            fake_scores = self.discriminator.classify_measurements(fake_measurements)
            w_distance = tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)
            
            # Gradient penalty on raw measurement space
            alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)
            interpolated = alpha * real_measurements + (1 - alpha) * fake_measurements
            
            with tf.GradientTape() as gp_tape:
                gp_tape.watch(interpolated)
                interp_scores = self.discriminator.classify_measurements(interpolated)
            
            gradients = gp_tape.gradient(interp_scores, interpolated)
            gradient_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1) + 1e-12)
            gradient_penalty = self.config['lambda_gp'] * tf.reduce_mean(tf.square(gradient_norm - 1.0))
            
            # Transformation regularization
            transform_reg = (self.generator.compute_transformation_regularization() + 
                           self.discriminator.compute_transformation_regularization())
            
            # Total discriminator loss
            d_loss = -w_distance + gradient_penalty + transform_reg
        
        # Apply discriminator gradients
        d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)
        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))
        
        # GENERATOR TRAINING
        with tf.GradientTape() as g_tape:
            # Generate fake data
            z = tf.random.normal([batch_size, self.config['latent_dim']])
            fake_data = self.generator.generate(z)
            
            # Get raw measurements and classify
            fake_measurements = self.discriminator.get_raw_measurements(fake_data)
            fake_scores = self.discriminator.classify_measurements(fake_measurements)
            
            # Generator loss (wants to fool discriminator)
            g_loss = -tf.reduce_mean(fake_scores) + transform_reg
        
        # Apply generator gradients
        g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)
        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))
        
        # Check gradient health
        g_grad_status = [g is not None for g in g_grads]
        d_grad_status = [g is not None for g in d_grads]
        
        return {
            'd_loss': float(d_loss),
            'g_loss': float(g_loss),
            'w_distance': float(w_distance),
            'gradient_penalty': float(gradient_penalty),
            'transform_reg': float(transform_reg),
            'g_grads_ok': all(g_grad_status),
            'd_grads_ok': all(d_grad_status),
            'g_grad_count': f"{sum(g_grad_status)}/{len(g_grad_status)}",
            'd_grad_count': f"{sum(d_grad_status)}/{len(d_grad_status)}"
        }
    
    def train(self, dataset=None):
        """
        Train the pure quantum GAN.
        
        Args:
            dataset: Training dataset (optional, will create bimodal if None)
        """
        logger.info("üèãÔ∏è Starting Pure Quantum GAN Training")
        
        # Create dataset if not provided
        if dataset is None:
            dataset = self.create_bimodal_dataset(n_samples=200)
            logger.info(f"üìä Created bimodal dataset: {dataset.shape}")
        
        # Training loop
        for epoch in range(self.config['epochs']):
            logger.info(f"\nüèÉ Epoch {epoch + 1}/{self.config['epochs']}")
            
            # Create batches
            dataset_batched = tf.data.Dataset.from_tensor_slices(dataset)
            dataset_batched = dataset_batched.batch(self.config['batch_size']).shuffle(1000)
            
            epoch_metrics = []
            
            # Train on batches
            for batch_idx, batch in enumerate(dataset_batched):
                metrics = self.train_step(batch)
                epoch_metrics.append(metrics)
                
                if batch_idx == 0:  # Log first batch of each epoch
                    logger.info(f"   üìä Batch {batch_idx + 1}: D={metrics['d_loss']:.4f}, G={metrics['g_loss']:.4f}")
                    logger.info(f"      Gradients: G={metrics['g_grad_count']}, D={metrics['d_grad_count']}")
            
            # Average epoch metrics
            avg_metrics = {}
            for key in epoch_metrics[0].keys():
                if key not in ['g_grads_ok', 'd_grads_ok', 'g_grad_count', 'd_grad_count']:
                    avg_metrics[key] = np.mean([m[key] for m in epoch_metrics])
                else:
                    avg_metrics[key] = epoch_metrics[-1][key]  # Use last batch status
            
            # Store history
            self.history['g_loss'].append(avg_metrics['g_loss'])
            self.history['d_loss'].append(avg_metrics['d_loss'])
            self.history['w_distance'].append(avg_metrics['w_distance'])
            self.history['gradient_penalty'].append(avg_metrics['gradient_penalty'])
            
            # Log epoch summary
            logger.info(f"   ‚úÖ Epoch {epoch + 1} complete:")
            logger.info(f"      D Loss: {avg_metrics['d_loss']:.4f}")
            logger.info(f"      G Loss: {avg_metrics['g_loss']:.4f}")
            logger.info(f"      W Distance: {avg_metrics['w_distance']:.4f}")
            logger.info(f"      Gradient Status: G={avg_metrics['g_grad_count']}, D={avg_metrics['d_grad_count']}")
        
        logger.info("üéâ Training completed!")
    
    def evaluate(self, n_samples=100):
        """Evaluate the trained generator."""
        logger.info(f"üìä Evaluating generator with {n_samples} samples")
        
        # Generate samples
        z = tf.random.normal([n_samples, self.config['latent_dim']])
        generated_samples = self.generator.generate(z)
        
        # Get raw measurements
        raw_measurements = self.generator.get_raw_measurements(z)
        
        # Compute statistics
        sample_stats = {
            'mean': tf.reduce_mean(generated_samples, axis=0).numpy(),
            'std': tf.math.reduce_std(generated_samples, axis=0).numpy(),
            'min': tf.reduce_min(generated_samples, axis=0).numpy(),
            'max': tf.reduce_max(generated_samples, axis=0).numpy(),
            'raw_measurement_dim': raw_measurements.shape[1]
        }
        
        logger.info(f"‚úÖ Evaluation complete:")
        logger.info(f"   Generated samples shape: {generated_samples.shape}")
        logger.info(f"   Sample mean: {sample_stats['mean']}")
        logger.info(f"   Sample std: {sample_stats['std']}")
        logger.info(f"   Raw measurements dim: {sample_stats['raw_measurement_dim']}")
        
        return generated_samples.numpy(), sample_stats
    
    def plot_results(self, real_data=None, generated_samples=None):
        """Plot training results and generated samples."""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Plot 1: Training losses
        axes[0, 0].plot(self.history['g_loss'], label='Generator', color='red')
        axes[0, 0].plot(self.history['d_loss'], label='Discriminator', color='blue')
        axes[0, 0].set_title('Training Losses')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Wasserstein distance
        axes[0, 1].plot(self.history['w_distance'], color='green')
        axes[0, 1].set_title('Wasserstein Distance')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Distance')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Generated samples vs real data
        if generated_samples is not None and real_data is not None:
            axes[1, 0].scatter(real_data[:, 0], real_data[:, 1], 
                             alpha=0.6, s=20, c='blue', label='Real Data')
            axes[1, 0].scatter(generated_samples[:, 0], generated_samples[:, 1], 
                             alpha=0.6, s=20, c='red', label='Generated')
            axes[1, 0].set_title('Real vs Generated Data')
            axes[1, 0].set_xlabel('Feature 1')
            axes[1, 0].set_ylabel('Feature 2')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 4: Gradient penalty
        axes[1, 1].plot(self.history['gradient_penalty'], color='orange')
        axes[1, 1].set_title('Gradient Penalty')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Penalty')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"pure_quantum_gan_results_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        logger.info(f"üìä Results saved to: {filename}")
        
        plt.show()
        
        return filename


def test_pure_quantum_gan():
    """Test the complete pure quantum GAN system."""
    print("\n" + "="*60)
    print("üöÄ TESTING COMPLETE PURE QUANTUM GAN SYSTEM")
    print("="*60)
    
    # Configuration
    config = {
        'latent_dim': 6,
        'output_dim': 2,
        'g_modes': 4,
        'd_modes': 2,
        'layers': 2,
        'cutoff_dim': 6,
        'g_lr': 0.002,
        'd_lr': 0.002,
        'batch_size': 8,
        'epochs': 5,  # Quick test
        'lambda_gp': 10.0,
        'lambda_transform': 1.0,
        'lambda_quantum': 0.5
    }
    
    # Create trainer
    trainer = PureQuantumGANTrainer(config)
    
    # Create test dataset
    real_data = trainer.create_bimodal_dataset(n_samples=100)
    
    # Train
    trainer.train(real_data)
    
    # Evaluate
    generated_samples, stats = trainer.evaluate(n_samples=50)
    
    # Plot results
    plot_file = trainer.plot_results(real_data.numpy(), generated_samples)
    
    print(f"\nüéâ Pure Quantum GAN test completed!")
    print(f"üìä Results visualization: {plot_file}")
    print(f"üî¨ Sample statistics: {stats}")
    
    return trainer


if __name__ == "__main__":
    trainer = test_pure_quantum_gan()