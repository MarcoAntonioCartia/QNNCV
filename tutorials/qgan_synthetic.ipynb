{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Generative Adversarial Networks with Continuous Variables\n",
    "\n",
    "## A Comprehensive Study of Pure Quantum Adversarial Learning\n",
    "\n",
    "This notebook presents a complete implementation and analysis of quantum generative adversarial networks using continuous variable quantum computing. We demonstrate the full pipeline from theoretical foundations to practical implementation, focusing on pure quantum architectures without classical components in the adversarial framework.\n",
    "\n",
    "### Research Objectives\n",
    "\n",
    "1. **Investigate quantum advantages** in generative modeling through continuous variable quantum computing\n",
    "2. **Analyze training dynamics** of pure quantum adversarial systems\n",
    "3. **Evaluate quantum expressivity** compared to classical baselines\n",
    "4. **Demonstrate practical implementation** of quantum GANs with synthetic data\n",
    "\n",
    "### Theoretical Framework\n",
    "\n",
    "Quantum GANs extend the classical adversarial framework to quantum systems, where both generator and discriminator operate in quantum Hilbert spaces. The continuous variable approach utilizes photonic quantum computing with infinite-dimensional Fock spaces, enabling rich quantum correlations through squeezing, displacement, and interferometric operations.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "min_G max_D V(D,G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]\n",
    "```\n",
    "\n",
    "Where G and D are quantum circuits operating on continuous variable quantum states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "We begin by importing necessary libraries and verifying the quantum computing environment. The implementation requires TensorFlow for automatic differentiation and Strawberry Fields for continuous variable quantum computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply compatibility patches for newer library versions\n",
    "print(\"Applying compatibility patches...\")\n",
    "\n",
    "# SciPy compatibility patch\n",
    "try:\n",
    "    import scipy.integrate\n",
    "    if not hasattr(scipy.integrate, 'simps'):\n",
    "        if hasattr(scipy.integrate, 'simpson'):\n",
    "            scipy.integrate.simps = scipy.integrate.simpson  # type: ignore[attr-defined]\n",
    "            print(\"Applied SciPy compatibility patch: simps -> simpson\")\n",
    "        else:\n",
    "            print(\"Warning: Neither simps nor simpson found in scipy.integrate\")\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not apply SciPy compatibility patch\")\n",
    "\n",
    "# Core scientific computing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "# TensorFlow for neural networks and automatic differentiation\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow imported successfully\")\n",
    "    \n",
    "    # Simple TensorFlow version check\n",
    "    try:\n",
    "        tf_version = str(tf.__version__) if hasattr(tf, '__version__') else \"Unknown\"\n",
    "        print(f\"TensorFlow version: {tf_version}\")\n",
    "    except:\n",
    "        print(\"TensorFlow version: Unknown\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import TensorFlow: {e}\")\n",
    "    tf = None\n",
    "\n",
    "# Quantum computing libraries\n",
    "try:\n",
    "    import strawberryfields as sf\n",
    "    from strawberryfields.ops import *\n",
    "    sf_version = getattr(sf, '__version__', 'Unknown')\n",
    "    print(f\"Strawberry Fields version: {sf_version}\")\n",
    "    QUANTUM_AVAILABLE = True\n",
    "    print(\"Quantum computing environment: Ready\")\n",
    "except ImportError as e:\n",
    "    print(f\"Quantum libraries not available: {e}\")\n",
    "    print(\"Falling back to classical simulation\")\n",
    "    QUANTUM_AVAILABLE = False\n",
    "    \n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set TensorFlow random seed safely\n",
    "if tf is not None:\n",
    "    try:\n",
    "        # Try different methods to set TensorFlow random seed\n",
    "        if hasattr(tf, 'random') and hasattr(tf.random, 'set_seed'):\n",
    "            tf.random.set_seed(42)\n",
    "            print(\"TensorFlow random seed set\")\n",
    "        elif hasattr(tf, 'set_random_seed'):\n",
    "            tf.set_random_seed(42)\n",
    "            print(\"TensorFlow random seed set (legacy method)\")\n",
    "        else:\n",
    "            print(\"TensorFlow random seed not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not set TensorFlow random seed: {e}\")\n",
    "\n",
    "# Configure matplotlib for high-quality plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENVIRONMENT CONFIGURATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"TensorFlow Available: {tf is not None}\")\n",
    "print(f\"Quantum Available: {QUANTUM_AVAILABLE}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import QNNCV Framework Components\n",
    "\n",
    "We import the quantum GAN components from our reorganized framework, ensuring proper path handling and fallback mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Apply compatibility patches before any framework imports\n",
    "print(\"Applying compatibility patches...\")\n",
    "\n",
    "# SciPy compatibility patch\n",
    "try:\n",
    "    import scipy.integrate\n",
    "    if not hasattr(scipy.integrate, 'simps'):\n",
    "        if hasattr(scipy.integrate, 'simpson'):\n",
    "            scipy.integrate.simps = scipy.integrate.simpson  # type: ignore[attr-defined]\n",
    "            print(\"Applied SciPy compatibility patch for framework imports\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# TensorFlow compatibility patch\n",
    "try:\n",
    "    from utils.tensorflow_compat import _patch_tensorflow_complex_ops\n",
    "    success = _patch_tensorflow_complex_ops()\n",
    "    if success:\n",
    "        print(\"TensorFlow compatibility patches applied successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Some TensorFlow compatibility issues remain\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import TensorFlow compatibility module: {e}\")\n",
    "\n",
    "print(\"Setting up enhanced warning suppression for quantum operations...\")\n",
    "\n",
    "# Suppress specific TensorFlow complex casting warnings\n",
    "warnings.filterwarnings('ignore', \n",
    "                       message='.*casting.*input.*type.*complex.*incompatible.*dtype.*float.*',\n",
    "                       category=UserWarning)\n",
    "\n",
    "warnings.filterwarnings('ignore', \n",
    "                       message='.*You are casting an input of type complex.*to an incompatible dtype.*',\n",
    "                       category=UserWarning)\n",
    "\n",
    "# Suppress TensorFlow logging warnings\n",
    "if tf is not None:\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    # Also suppress specific TensorFlow warnings\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages\n",
    "\n",
    "# Create a one-time warning notifier for complex casting\n",
    "class ComplexCastingNotifier:\n",
    "    def __init__(self):\n",
    "        self.notified = False\n",
    "    \n",
    "    def notify_once(self):\n",
    "        if not self.notified:\n",
    "            print(\"  Note: Complex number casting warnings have been suppressed for cleaner output\")\n",
    "            print(\"   (This is normal for quantum operations - imaginary parts are handled correctly)\")\n",
    "            self.notified = True\n",
    "\n",
    "complex_notifier = ComplexCastingNotifier()\n",
    "complex_notifier.notify_once()\n",
    "\n",
    "print(\"Warning suppression configured successfully ✅\")\n",
    "\n",
    "try:\n",
    "    # Import quantum GAN components\n",
    "    from models.generators.quantum_differentiable_generator import QuantumDifferentiableGenerator\n",
    "    from models.discriminators.quantum_continuous_discriminator import QuantumContinuousDiscriminator\n",
    "    from training.qgan_trainer import QGAN\n",
    "    \n",
    "    # Import utility functions\n",
    "    from utils.data_utils import load_synthetic_data, create_output_directory\n",
    "    from utils.visualization import plot_results, plot_training_history\n",
    "    from utils.metrics import (\n",
    "        compute_wasserstein_distance, \n",
    "        compute_mmd, \n",
    "        compute_coverage_and_precision\n",
    "    )\n",
    "    from utils.quantum_gradients import (\n",
    "        create_quantum_generator_with_gradients,\n",
    "        create_quantum_discriminator_with_gradients\n",
    "    )\n",
    "    \n",
    "    print(\"QNNCV framework components imported successfully :)\")\n",
    "    FRAMEWORK_AVAILABLE = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Framework import error: {e}\")\n",
    "    print(\"Please ensure the src directory structure is correct\")\n",
    "    FRAMEWORK_AVAILABLE = False\n",
    "\n",
    "# Verify complete environment\n",
    "ENVIRONMENT_READY = QUANTUM_AVAILABLE and FRAMEWORK_AVAILABLE\n",
    "print(f\"\\nComplete environment status: {'Ready for quantum experiments :)' if ENVIRONMENT_READY else 'Limited functionality available :('}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthetic Data Generation and Analysis\n",
    "\n",
    "We generate synthetic datasets specifically designed to test quantum GAN capabilities. The datasets are chosen to highlight potential quantum advantages in capturing complex distributions and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_research_datasets(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Generate synthetic datasets for quantum GAN research.\n",
    "    \n",
    "    Returns datasets designed to test different aspects of quantum expressivity:\n",
    "    - Gaussian mixtures: Multi-modal distributions\n",
    "    - Spiral patterns: Non-linear correlations\n",
    "    - Ring distributions: Circular symmetries\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Two-dimensional Gaussian mixture\n",
    "    np.random.seed(42)\n",
    "    n_per_mode = n_samples // 4\n",
    "    \n",
    "    # Four Gaussian modes in different quadrants\n",
    "    modes = [(-2, -2), (2, -2), (-2, 2), (2, 2)]\n",
    "    gaussian_data = []\n",
    "    \n",
    "    for mode in modes:\n",
    "        samples = np.random.multivariate_normal(\n",
    "            mean=mode, \n",
    "            cov=[[0.3, 0.1], [0.1, 0.3]], \n",
    "            size=n_per_mode\n",
    "        )\n",
    "        gaussian_data.append(samples)\n",
    "    \n",
    "    datasets['gaussian_mixture'] = np.vstack(gaussian_data)\n",
    "    \n",
    "    # 2. Spiral pattern (tests non-linear correlations)\n",
    "    t = np.linspace(0, 4*np.pi, n_samples)\n",
    "    r = t / (4*np.pi) * 3\n",
    "    spiral_x = r * np.cos(t) + np.random.normal(0, 0.1, n_samples)\n",
    "    spiral_y = r * np.sin(t) + np.random.normal(0, 0.1, n_samples)\n",
    "    datasets['spiral'] = np.column_stack([spiral_x, spiral_y])\n",
    "    \n",
    "    # 3. Ring distribution (tests circular symmetries)\n",
    "    angles = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    radius = np.random.normal(2.0, 0.2, n_samples)\n",
    "    ring_x = radius * np.cos(angles)\n",
    "    ring_y = radius * np.sin(angles)\n",
    "    datasets['ring'] = np.column_stack([ring_x, ring_y])\n",
    "    \n",
    "    # 4. Moons dataset (sklearn)\n",
    "    moons_data, _ = make_moons(n_samples=n_samples, noise=0.1, random_state=42)\n",
    "    datasets['moons'] = moons_data * 2  # Scale for better visualization\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def analyze_dataset_properties(data, name):\n",
    "    \"\"\"\n",
    "    Compute statistical properties of the dataset for baseline comparison.\n",
    "    \"\"\"\n",
    "    properties = {\n",
    "        'name': name,\n",
    "        'n_samples': len(data),\n",
    "        'dimensionality': data.shape[1],\n",
    "        'mean': np.mean(data, axis=0),\n",
    "        'std': np.std(data, axis=0),\n",
    "        'correlation': np.corrcoef(data.T)[0, 1] if data.shape[1] == 2 else None,\n",
    "        'range_x': (np.min(data[:, 0]), np.max(data[:, 0])),\n",
    "        'range_y': (np.min(data[:, 1]), np.max(data[:, 1])) if data.shape[1] >= 2 else None\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating synthetic datasets for quantum GAN research...\")\n",
    "datasets = generate_research_datasets(n_samples=2000)\n",
    "\n",
    "# Analyze dataset properties\n",
    "dataset_properties = {}\n",
    "for name, data in datasets.items():\n",
    "    properties = analyze_dataset_properties(data, name)\n",
    "    dataset_properties[name] = properties\n",
    "    print(f\"\\n{name.upper()} Dataset:\")\n",
    "    print(f\"  Samples: {properties['n_samples']}\")\n",
    "    print(f\"  Mean: [{properties['mean'][0]:.3f}, {properties['mean'][1]:.3f}]\")\n",
    "    print(f\"  Std: [{properties['std'][0]:.3f}, {properties['std'][1]:.3f}]\")\n",
    "    print(f\"  Correlation: {properties['correlation']:.3f}\")\n",
    "\n",
    "print(\"\\nDataset generation and analysis complete :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Generated Datasets\n",
    "\n",
    "We visualize the synthetic datasets to understand their distributional characteristics and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of all datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "dataset_names = list(datasets.keys())\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot with density information\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], \n",
    "                        c=colors[idx], alpha=0.6, s=20, \n",
    "                        label=f'{name.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    # Add contour lines for density estimation\n",
    "    try:\n",
    "        from scipy.stats import gaussian_kde\n",
    "        kde = gaussian_kde(data.T)\n",
    "        x_range = np.linspace(data[:, 0].min(), data[:, 0].max(), 50)\n",
    "        y_range = np.linspace(data[:, 1].min(), data[:, 1].max(), 50)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "        Z = kde(positions).reshape(X.shape)\n",
    "        ax.contour(X, Y, Z, levels=5, colors='black', alpha=0.3, linewidths=0.5)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ax.set_title(f'{name.replace(\"_\", \" \").title()} Distribution\\n'\n",
    "                f'N={len(data)}, Corr={dataset_properties[name][\"correlation\"]:.3f}',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('X₁', fontsize=12)\n",
    "    ax.set_ylabel('X₂', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Synthetic Datasets for Quantum GAN Research', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Dataset visualization complete. Each dataset presents unique challenges:\")\n",
    "print(\"- Gaussian Mixture: Multi-modal distribution learning\")\n",
    "print(\"- Spiral: Non-linear correlation capture\")\n",
    "print(\"- Ring: Circular symmetry and radial structure\")\n",
    "print(\"- Moons: Non-convex decision boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pure Quantum GAN Architecture\n",
    "\n",
    "We implement a pure quantum GAN where both generator and discriminator are quantum circuits. This represents the most advanced form of quantum adversarial learning, utilizing the full expressivity of quantum systems.\n",
    "\n",
    "### Quantum Generator Architecture\n",
    "\n",
    "The quantum generator employs continuous variable quantum computing with:\n",
    "- **Classical encoding network**: Maps latent vectors to quantum parameters\n",
    "- **Two-mode squeezing**: Creates entanglement between quantum modes\n",
    "- **Interferometer**: Enables quantum interference and mode coupling\n",
    "- **Displacement gates**: Encodes classical information into quantum states\n",
    "- **Adaptive measurements**: Extracts classical outputs from quantum states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pure_quantum_gan(n_qumodes=4, latent_dim=8, cutoff_dim=8):\n",
    "    \"\"\"\n",
    "    Create a pure quantum GAN with both generator and discriminator as quantum circuits.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_qumodes: Number of quantum modes (affects expressivity and computational cost)\n",
    "    - latent_dim: Dimensionality of classical latent input\n",
    "    - cutoff_dim: Fock space truncation (affects simulation accuracy)\n",
    "    \n",
    "    Returns:\n",
    "    - Configured QGAN instance with quantum components\n",
    "    \"\"\"\n",
    "    \n",
    "    if not ENVIRONMENT_READY:\n",
    "        print(\"Environment not ready for quantum GAN creation :(\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Creating pure quantum GAN architecture...\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Quantum modes: {n_qumodes}\")\n",
    "    print(f\"  - Latent dimension: {latent_dim}\")\n",
    "    print(f\"  - Cutoff dimension: {cutoff_dim}\")\n",
    "    print(f\"  - Hilbert space size: {cutoff_dim**n_qumodes}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize quantum generator\n",
    "        print(\"\\nInitializing quantum generator...\")\n",
    "        generator = create_quantum_generator_with_gradients(\n",
    "            QuantumDifferentiableGenerator,\n",
    "            n_qumodes=2,\n",
    "            latent_dim=latent_dim,\n",
    "            cutoff_dim=cutoff_dim,\n",
    "            use_quantum=True  # Enable quantum operations\n",
    "        )\n",
    "        \n",
    "        # Test generator functionality\n",
    "        test_z = tf.random.normal([2, latent_dim])\n",
    "        test_output = generator.generate(test_z)\n",
    "        print(f\"Generator test successful: {test_output.shape} :)\")\n",
    "        \n",
    "        # Initialize quantum discriminator\n",
    "        print(\"\\nInitializing quantum discriminator...\")\n",
    "        discriminator = create_quantum_discriminator_with_gradients(\n",
    "            QuantumContinuousDiscriminator,\n",
    "            n_qumodes=2,\n",
    "            input_dim=2,  # Matches generator output\n",
    "            cutoff_dim=cutoff_dim\n",
    "        )\n",
    "        \n",
    "        # Test discriminator functionality\n",
    "        test_probs = discriminator.discriminate(test_output)\n",
    "        print(f\"Discriminator test successful: {test_probs.shape} :)\")\n",
    "        \n",
    "        # Create QGAN with quantum-optimized parameters\n",
    "        print(\"\\nAssembling quantum GAN framework...\")\n",
    "        qgan = QGAN(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            latent_dim=latent_dim,\n",
    "            generator_lr=5e-5,      # Lower learning rate for quantum stability\n",
    "            discriminator_lr=5e-5,   # Matched learning rates for balance\n",
    "            beta1=0.5,              # Standard GAN beta1\n",
    "            beta2=0.999,            # Standard GAN beta2\n",
    "            gradient_clip_norm=0.5   # Aggressive clipping for quantum circuits\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPure quantum GAN created successfully :)\")\n",
    "        print(f\"Total trainable parameters:\")\n",
    "        print(f\"  - Generator: {len(generator.trainable_variables)}\")\n",
    "        print(f\"  - Discriminator: {len(discriminator.trainable_variables)}\")\n",
    "        \n",
    "        return qgan\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating quantum GAN: {e} :(\")\n",
    "        return None\n",
    "\n",
    "# Create the pure quantum GAN\n",
    "print(\"=\" * 60)\n",
    "print(\"PURE QUANTUM GAN ARCHITECTURE INITIALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quantum_gan = create_pure_quantum_gan(\n",
    "    n_qumodes=4,      # Manageable for simulation\n",
    "    latent_dim=8,     # Sufficient for 2D data generation\n",
    "    cutoff_dim=8      # Balance between accuracy and computation\n",
    ")\n",
    "\n",
    "if quantum_gan is not None:\n",
    "    print(\"\\nQuantum GAN architecture ready for training experiments :)\")\n",
    "else:\n",
    "    print(\"\\nFailed to create quantum GAN architecture :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Hyperparameter Analysis\n",
    "\n",
    "Quantum GANs require careful hyperparameter tuning due to the sensitivity of quantum circuits to parameter updates. We analyze the training configuration and establish monitoring protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_quantum_training(dataset_name='spiral', n_epochs=5, batch_size=16):\n",
    "    \"\"\"\n",
    "    Configure training parameters optimized for quantum GAN stability.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name: Which synthetic dataset to use\n",
    "    - n_epochs: Number of training epochs\n",
    "    - batch_size: Batch size (smaller for quantum stability)\n",
    "    \n",
    "    Returns:\n",
    "    - Training configuration dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset_name not in datasets:\n",
    "        print(f\"Dataset {dataset_name} not available. Using spiral instead.\")\n",
    "        dataset_name = 'spiral'\n",
    "    \n",
    "    # Get and preprocess the selected dataset\n",
    "    raw_data = datasets[dataset_name]\n",
    "    \n",
    "    # Normalize data for quantum circuit stability\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(raw_data)\n",
    "    \n",
    "    # Further scale to quantum-friendly range [-1, 1]\n",
    "    data_range = np.max(np.abs(normalized_data))\n",
    "    if data_range > 0:\n",
    "        normalized_data = normalized_data / data_range\n",
    "    \n",
    "    # Convert to TensorFlow tensor\n",
    "    training_data = tf.constant(normalized_data, dtype=tf.float32)\n",
    "    \n",
    "    config = {\n",
    "        'dataset_name': dataset_name,\n",
    "        '
