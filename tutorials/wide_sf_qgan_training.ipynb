{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a52cbeb",
   "metadata": {},
   "source": [
    "# Extended SF Quantum GAN Training with Comprehensive Monitoring\n",
    "\n",
    "This notebook demonstrates extended training of the minimal SF quantum GAN with:\n",
    "- **Extended Training**: 100+ epochs for proper convergence\n",
    "- **Quality Tracking**: Real-time monitoring of generation quality\n",
    "- **Comprehensive Visualization**: Training evolution dashboard\n",
    "- **Performance Analysis**: Detailed convergence analysis\n",
    "\n",
    "Based on the minimal results showing poor quality (Mean diff: 2.8244), we need much longer training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320025f7",
   "metadata": {},
   "source": [
    "## 1. Setup and Enhanced Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0f303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training environment initialized\n",
      "Warnings suppressed, ready for quantum training\n",
      "Extended training environment setup complete ✓\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Configure matplotlib for Jupyter\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import our clean training utilities\n",
    "from utils.warning_suppression import enable_clean_training\n",
    "\n",
    "# Enable clean output\n",
    "enable_clean_training()\n",
    "\n",
    "print(\"Extended training environment setup complete ✓\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b70344",
   "metadata": {},
   "source": [
    "## 2. Import Quantum Components and Create Enhanced Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b504e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum components imported successfully ✓\n",
      "Enhanced trainer class created ✓\n"
     ]
    }
   ],
   "source": [
    "# Import our SF-based quantum components\n",
    "from models.generators.quantum_sf_generator import QuantumSFGenerator\n",
    "from models.discriminators.quantum_sf_discriminator import QuantumSFDiscriminator\n",
    "from training.qgan_sf_trainer import QGANSFTrainer\n",
    "\n",
    "print(\"Quantum components imported successfully ✓\")\n",
    "\n",
    "class ExtendedQGANTrainer(QGANSFTrainer):\n",
    "    \"\"\"\n",
    "    Enhanced trainer with quality monitoring during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quality_history = {\n",
    "            'epochs': [],\n",
    "            'mean_differences': [],\n",
    "            'std_differences': [],\n",
    "            'wasserstein_distances': [],\n",
    "            'generator_losses': [],\n",
    "            'discriminator_losses': [],\n",
    "            'stability_metrics': [],\n",
    "            'training_times': []\n",
    "        }\n",
    "    \n",
    "    def compute_quality_metrics(self, real_data, n_samples=200):\n",
    "        \"\"\"\n",
    "        Compute comprehensive quality metrics.\n",
    "        \"\"\"\n",
    "        # Generate samples\n",
    "        z = tf.random.normal([n_samples, self.latent_dim])\n",
    "        generated_samples = self.generator.generate(z)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        real_np = real_data.numpy() if hasattr(real_data, 'numpy') else real_data\n",
    "        gen_np = generated_samples.numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        real_mean = np.mean(real_np, axis=0)\n",
    "        gen_mean = np.mean(gen_np, axis=0)\n",
    "        real_std = np.std(real_np, axis=0)\n",
    "        gen_std = np.std(gen_np, axis=0)\n",
    "        \n",
    "        mean_diff = np.linalg.norm(real_mean - gen_mean)\n",
    "        std_diff = np.linalg.norm(real_std - gen_std)\n",
    "        \n",
    "        # Wasserstein distance (1D approximation)\n",
    "        try:\n",
    "            wd = wasserstein_distance(real_np[:, 0], gen_np[:, 0])\n",
    "        except:\n",
    "            wd = float('inf')\n",
    "        \n",
    "        return {\n",
    "            'mean_difference': mean_diff,\n",
    "            'std_difference': std_diff,\n",
    "            'wasserstein_distance': wd,\n",
    "            'generated_samples': gen_np\n",
    "        }\n",
    "    \n",
    "    def train_with_monitoring(self, data, epochs=100, batch_size=8, \n",
    "                            monitor_interval=5, verbose=True):\n",
    "        \"\"\"\n",
    "        Train with comprehensive monitoring.\n",
    "        \"\"\"\n",
    "        print(f\"Starting training: {epochs} epochs\")\n",
    "        print(f\"Monitoring every {monitor_interval} epochs\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initial quality assessment\n",
    "        initial_quality = self.compute_quality_metrics(data)\n",
    "        print(f\"Initial quality - Mean diff: {initial_quality['mean_difference']:.4f}\")\n",
    "        \n",
    "        # Training loop with monitoring\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Train one epoch\n",
    "            epoch_history = self._train_single_epoch(data, batch_size)\n",
    "            \n",
    "            # Monitor quality at intervals\n",
    "            if epoch % monitor_interval == 0 or epoch == epochs - 1:\n",
    "                quality_metrics = self.compute_quality_metrics(data)\n",
    "                \n",
    "                # Store metrics\n",
    "                self.quality_history['epochs'].append(epoch)\n",
    "                self.quality_history['mean_differences'].append(quality_metrics['mean_difference'])\n",
    "                self.quality_history['std_differences'].append(quality_metrics['std_difference'])\n",
    "                self.quality_history['wasserstein_distances'].append(quality_metrics['wasserstein_distance'])\n",
    "                self.quality_history['generator_losses'].append(epoch_history['g_loss'])\n",
    "                self.quality_history['discriminator_losses'].append(epoch_history['d_loss'])\n",
    "                self.quality_history['stability_metrics'].append(epoch_history['stability'])\n",
    "                self.quality_history['training_times'].append(time.time() - epoch_start)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:3d}: G_loss={epoch_history['g_loss']:.4f}, \"\n",
    "                          f\"D_loss={epoch_history['d_loss']:.4f}, \"\n",
    "                          f\"Mean_diff={quality_metrics['mean_difference']:.4f}, \"\n",
    "                          f\"WD={quality_metrics['wasserstein_distance']:.4f}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nCORRECTED training completed in {total_time:.1f}s\")\n",
    "        \n",
    "        return self.quality_history\n",
    "    \n",
    "    def _train_single_epoch(self, data, batch_size):\n",
    "        \"\"\"\n",
    "        Train for one epoch and return metrics.\n",
    "        \"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size).shuffle(1000)\n",
    "        \n",
    "        epoch_g_losses = []\n",
    "        epoch_d_losses = []\n",
    "        \n",
    "        for batch in dataset:\n",
    "            # Train discriminator\n",
    "            d_loss = self._train_discriminator_step(batch)\n",
    "            \n",
    "            # Train generator\n",
    "            g_loss = self._train_generator_step(batch.shape[0])\n",
    "            \n",
    "            epoch_g_losses.append(float(g_loss))\n",
    "            epoch_d_losses.append(float(d_loss))\n",
    "        \n",
    "        avg_g_loss = np.mean(epoch_g_losses)\n",
    "        avg_d_loss = np.mean(epoch_d_losses)\n",
    "        stability = avg_g_loss / (avg_d_loss + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            'g_loss': avg_g_loss,\n",
    "            'd_loss': avg_d_loss,\n",
    "            'stability': stability\n",
    "        }\n",
    "    \n",
    "    def _train_discriminator_step(self, real_batch):\n",
    "        \"\"\"\n",
    "        Discriminator training step  with  engine reset outside the loop!\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = tf.shape(real_batch)[0]\n",
    "\n",
    "        # Reset engines BEFORE gradient computation (like SF tutorial)\n",
    "        if self.generator.eng.run_progs:\n",
    "            self.generator.eng.reset()\n",
    "        if self.discriminator.eng.run_progs:\n",
    "            self.discriminator.eng.reset()\n",
    "\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            # Real samples\n",
    "            real_output = self.discriminator.discriminate(real_batch)\n",
    "            \n",
    "            # Fake samples\n",
    "            z = tf.random.normal([batch_size, self.latent_dim])\n",
    "            fake_samples = self.generator.generate(z)\n",
    "            fake_output = self.discriminator.discriminate(fake_samples)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            d_loss = -tf.reduce_mean(tf.math.log(real_output + 1e-8) + \n",
    "                                   tf.math.log(1 - fake_output + 1e-8))\n",
    "        \n",
    "        # Compute gradients - ONLY for discriminator from this tape\n",
    "        gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        # Filter out None gradients, this could break training\n",
    "        valid_gradients = []\n",
    "        valid_variables = []\n",
    "        \n",
    "        for grad, var in zip(gradients, self.discriminator.trainable_variables):\n",
    "            if grad is not None:\n",
    "                valid_gradients.append(grad)\n",
    "                valid_variables.append(var)\n",
    "        \n",
    "        # Apply gradients\n",
    "        if valid_gradients:\n",
    "            self.d_optimizer.apply_gradients(zip(valid_gradients, valid_variables))\n",
    "            print(f\"Applied {len(valid_gradients)}/{len(gradients)} discriminator gradients\")\n",
    "        else:\n",
    "            print(\"WARNING: No valid discriminator gradients - skipping update\")\n",
    "\n",
    "        return d_loss\n",
    "    \n",
    "    def _train_generator_step(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generator training step\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            z = tf.random.normal([batch_size, self.latent_dim])\n",
    "            fake_samples = self.generator.generate(z)\n",
    "            fake_output = self.discriminator.discriminate(fake_samples)\n",
    "            \n",
    "            # Generator loss\n",
    "            g_loss = -tf.reduce_mean(tf.math.log(fake_output + 1e-8))\n",
    "        \n",
    "        # Compute gradients - ONLY for generator from this tape\n",
    "        gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        \n",
    "        # Filter out None gradients\n",
    "        valid_gradients = []\n",
    "        valid_variables = []\n",
    "        \n",
    "        for grad, var in zip(gradients, self.generator.trainable_variables):\n",
    "            if grad is not None:\n",
    "                valid_gradients.append(grad)\n",
    "                valid_variables.append(var)\n",
    "        \n",
    "        # Apply gradients\n",
    "        if valid_gradients:\n",
    "            self.g_optimizer.apply_gradients(zip(valid_gradients, valid_variables))\n",
    "            print(f\"Applied {len(valid_gradients)}/{len(gradients)} generator gradients\")\n",
    "        else:\n",
    "            print(\"WARNING: No valid generator gradients - skipping update\")\n",
    "            \n",
    "        return g_loss\n",
    "\n",
    "print(\"Enhanced trainer class created ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6044",
   "metadata": {},
   "source": [
    "## 3. Create Data and Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187fb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created: (500, 2)\n",
      "Data range: [-0.839, 1.000]\n",
      "\n",
      "Creating enhanced quantum components...\n",
      "✓ Enhanced generator: 18 trainable variables\n",
      "✓ Enhanced discriminator: 9 trainable variables\n",
      "✓ Enhanced trainer created\n",
      "  Total parameters: 27\n"
     ]
    }
   ],
   "source": [
    "def create_simple_2d_data(n_samples=500):\n",
    "    \"\"\"\n",
    "    Create simple 2D Gaussian mixture for testing.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Two Gaussian clusters\n",
    "    cluster1 = np.random.normal([1.0, 1.0], 0.3, (n_samples//2, 2))\n",
    "    cluster2 = np.random.normal([-1.0, -1.0], 0.3, (n_samples//2, 2))\n",
    "    \n",
    "    data = np.vstack([cluster1, cluster2])\n",
    "    \n",
    "    # Normalize to [-1, 1] range for quantum stability\n",
    "    data = data / np.max(np.abs(data))\n",
    "    \n",
    "    return tf.constant(data, dtype=tf.float32)\n",
    "\n",
    "# Generate data\n",
    "real_data = create_simple_2d_data(n_samples=500)\n",
    "print(f\"Data created: {real_data.shape}\")\n",
    "print(f\"Data range: [{tf.reduce_min(real_data):.3f}, {tf.reduce_max(real_data):.3f}]\")\n",
    "\n",
    "# Create quantum components with slightly improved settings\n",
    "print(\"\\nCreating enhanced quantum components...\")\n",
    "\n",
    "# Generator with 2 modes for better expressivity\n",
    "generator = QuantumSFGenerator(\n",
    "    n_modes=2,        # 2 modes for richer quantum correlations\n",
    "    latent_dim=2,     # 2D latent input\n",
    "    layers=1,         # Still minimal layers\n",
    "    cutoff_dim=8      # Higher cutoff for better precision\n",
    ")\n",
    "\n",
    "# Discriminator with 1 mode (minimal)\n",
    "discriminator = QuantumSFDiscriminator(\n",
    "    n_modes=1,        # 1 quantum mode (minimal)\n",
    "    input_dim=2,      # 2D input data\n",
    "    layers=1,         # 1 quantum layer (minimal)\n",
    "    cutoff_dim=8      # Higher cutoff for better precision\n",
    ")\n",
    "\n",
    "print(f\"✓ Enhanced generator: {len(generator.trainable_variables)} trainable variables\")\n",
    "print(f\"✓ Enhanced discriminator: {len(discriminator.trainable_variables)} trainable variables\")\n",
    "\n",
    "# Create enhanced trainer\n",
    "trainer = ExtendedQGANTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    latent_dim=2,\n",
    "    generator_lr=5e-4,      # Slightly lower learning rate for stability\n",
    "    discriminator_lr=5e-4,  # Matched learning rates\n",
    "    beta1=0.5,\n",
    "    beta2=0.999\n",
    ")\n",
    "\n",
    "print(f\"✓ Enhanced trainer created\")\n",
    "print(f\"  Total parameters: {len(generator.trainable_variables) + len(discriminator.trainable_variables)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e99ab8",
   "metadata": {},
   "source": [
    "## 4. Extended Training with Real-Time Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e54790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extended quantum GAN training with comprehensive monitoring...\n",
      "This will take significantly longer but should show real learning!\n",
      "Starting training: 100 epochs\n",
      "Monitoring every 5 epochs\n",
      "Data shape: (500, 2)\n",
      "Initial quality - Mean diff: 2.8202\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Epoch   0: G_loss=0.3936, D_loss=1.5170, Mean_diff=2.8252, WD=1.9914\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Epoch   5: G_loss=0.5689, D_loss=1.3992, Mean_diff=2.5954, WD=1.8339\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n",
      "Applied 18/18 generator gradients\n",
      "Applied 9/9 discriminator gradients\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2348\\1862847976.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33m'Could not get source, probably due dynamically evaluated source code.'\u001b[39m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2348\\1434378633.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, epochs, batch_size, monitor_interval, verbose)\u001b[39m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2348\\1434378633.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, batch_size)\u001b[39m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2348\\1434378633.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n",
      "\u001b[32mc:\\Users\\MendMa1\\Documents\\Personal\\Thesis\\github\\QNNCV\\src\\models\\generators\\quantum_sf_generator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, z)\u001b[39m\n\u001b[32m    391\u001b[39m         batch_size = tf.shape(z)[\u001b[32m0\u001b[39m]\n\u001b[32m    392\u001b[39m \n\u001b[32m    393\u001b[39m         \u001b[38;5;66;03m# Use quantum encoding strategy if available\u001b[39;00m\n\u001b[32m    394\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.quantum_encoder \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._generate_with_quantum_encoding(z)\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    397\u001b[39m             \u001b[38;5;66;03m# Fallback to classical encoding\u001b[39;00m\n\u001b[32m    398\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._generate_with_classical_encoding(z)\n",
      "\u001b[32mc:\\Users\\MendMa1\\Documents\\Personal\\Thesis\\github\\QNNCV\\src\\models\\generators\\quantum_sf_generator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, z)\u001b[39m\n\u001b[32m    415\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._generate_with_parameter_modulation(z, encoded_params)\n\u001b[32m    416\u001b[39m \n\u001b[32m    417\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m             \u001b[38;5;66;03m# Fallback to classical encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._generate_with_classical_encoding(z)\n",
      "\u001b[32mc:\\Users\\MendMa1\\Documents\\Personal\\Thesis\\github\\QNNCV\\src\\models\\generators\\quantum_sf_generator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, z)\u001b[39m\n\u001b[32m    426\u001b[39m         encoded_params = self.encoder(z)  \u001b[38;5;66;03m# [batch_size, num_quantum_params]\u001b[39;00m\n\u001b[32m    427\u001b[39m \n\u001b[32m    428\u001b[39m         \u001b[38;5;66;03m# Process samples with batch optimization if enabled\u001b[39;00m\n\u001b[32m    429\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.enable_batch_processing \u001b[38;5;28;01mand\u001b[39;00m batch_size <= \u001b[32m8\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._generate_batch_optimized(encoded_params)\n\u001b[32m    431\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    432\u001b[39m             \u001b[38;5;66;03m# Sequential processing (original method)\u001b[39;00m\n\u001b[32m    433\u001b[39m             all_samples = []\n",
      "\u001b[32mc:\\Users\\MendMa1\\Documents\\Personal\\Thesis\\github\\QNNCV\\src\\models\\generators\\quantum_sf_generator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, encoded_params)\u001b[39m\n\u001b[32m    499\u001b[39m \n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# Quantum operations still sequential (SF limitation)\u001b[39;00m\n\u001b[32m    501\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m tf.device(\u001b[33m'/CPU:0'\u001b[39m):  \u001b[38;5;66;03m# Force CPU for quantum operations\u001b[39;00m\n\u001b[32m    502\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m range(batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m                 sample = self._generate_single(processed_params[i])\n\u001b[32m    504\u001b[39m                 all_samples.append(sample)\n\u001b[32m    505\u001b[39m \n\u001b[32m    506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.stack(all_samples, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[32mc:\\Users\\MendMa1\\Documents\\Personal\\Thesis\\github\\QNNCV\\src\\models\\generators\\quantum_sf_generator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, quantum_params)\u001b[39m\n\u001b[32m    652\u001b[39m             samples = self._extract_samples_from_state(state)\n\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m samples\n\u001b[32m    655\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    657\u001b[39m             logger.debug(\u001b[33mf\"Quantum circuit failed: {e}\"\u001b[39m)\n\u001b[32m    658\u001b[39m             \u001b[38;5;66;03m# Classical fallback\u001b[39;00m\n\u001b[32m    659\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m tf.random.normal([self.n_modes], stddev=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, program, args, compile_options, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m                     raise NotImplementedError(\n\u001b[32m    567\u001b[39m                         \u001b[33m\"Feed-forwarding of measurements cannot be used together with multiple shots.\"\u001b[39m\n\u001b[32m    568\u001b[39m                     )\n\u001b[32m    569\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m         return super()._run(\n\u001b[32m    571\u001b[39m             program_lst, args=args, compile_options=compile_options, **eng_run_options\n\u001b[32m    572\u001b[39m         )\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, program, args, compile_options, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m             \u001b[38;5;66;03m# bind free parameters to their values\u001b[39;00m\n\u001b[32m    303\u001b[39m             p.bind_params(args)\n\u001b[32m    304\u001b[39m             p.lock()\n\u001b[32m    305\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m             _, self.samples, self.samples_dict = self._run_program(p, **kwargs)\n\u001b[32m    307\u001b[39m             self.run_progs.append(p)\n\u001b[32m    308\u001b[39m \n\u001b[32m    309\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m isinstance(p, TDMProgram) \u001b[38;5;28;01mand\u001b[39;00m received_rolled:\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, prog, **kwargs)\u001b[39m\n\u001b[32m    448\u001b[39m                 raise NotApplicableError(\n\u001b[32m    449\u001b[39m                     \u001b[33mf\"The operation {cmd.op} cannot be used with {self.backend}.\"\u001b[39m\n\u001b[32m    450\u001b[39m                 ) from None\n\u001b[32m    451\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m NotImplementedError:\n\u001b[32m    453\u001b[39m                 \u001b[38;5;66;03m# command not directly supported by backend API\u001b[39;00m\n\u001b[32m    454\u001b[39m                 raise NotImplementedError(\n\u001b[32m    455\u001b[39m                     \u001b[33mf\"The operation {cmd.op} has not been implemented in {self.backend} for the \"\u001b[39m\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, reg, backend, **kwargs)\u001b[39m\n\u001b[32m    504\u001b[39m \n\u001b[32m    505\u001b[39m         \u001b[38;5;66;03m# convert RegRefs back to indices for the backend API\u001b[39;00m\n\u001b[32m    506\u001b[39m         temp = [rr.ind \u001b[38;5;28;01mfor\u001b[39;00m rr \u001b[38;5;28;01min\u001b[39;00m reg]\n\u001b[32m    507\u001b[39m         \u001b[38;5;66;03m# call the child class specialized _apply method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         self._apply(temp, backend, **kwargs)\n\u001b[32m    509\u001b[39m         self.p[\u001b[32m0\u001b[39m] = original_p0  \u001b[38;5;66;03m# restore the original Parameter instance\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, reg, backend, **kwargs)\u001b[39m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _apply(self, reg, backend, **kwargs):\n\u001b[32m   1813\u001b[39m         p = par_evaluate(self.p)\n\u001b[32m-> \u001b[39m\u001b[32m1814\u001b[39m         backend.kerr_interaction(p[\u001b[32m0\u001b[39m], *reg)\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\backends\\tfbackend\\backend.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, kappa, mode)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m kerr_interaction(self, kappa, mode):\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m tf.name_scope(\u001b[33m\"Kerr_interaction\"\u001b[39m):\n\u001b[32m    223\u001b[39m             remapped_mode = self._remap_modes(mode)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m             self.circuit.kerr_interaction(kappa, remapped_mode)\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\strawberryfields\\backends\\tfbackend\\circuit.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, kappa, mode)\u001b[39m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m kerr_interaction(self, kappa, mode):\n\u001b[32m    538\u001b[39m         \"\"\"\n\u001b[32m    539\u001b[39m         Apply the Kerr interaction operator to the specified mode.\n\u001b[32m    540\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m         k = tf.cast(kappa, self._dtype)\n\u001b[32m    542\u001b[39m         k = self._maybe_batch(k)\n\u001b[32m    543\u001b[39m         new_state = ops.kerr_interaction(\n\u001b[32m    544\u001b[39m             k, mode, self._state, self._cutoff_dim, self._state_is_pure, self._batched\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, dtype, name)\u001b[39m\n\u001b[32m   1017\u001b[39m             \u001b[33m\"discard the imaginary part and may not be what you \"\u001b[39m\n\u001b[32m   1018\u001b[39m             \u001b[33m\"intended.\"\u001b[39m\n\u001b[32m   1019\u001b[39m         )\n\u001b[32m   1020\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m x.dtype != base_type:\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m         x = gen_math_ops.cast(x, base_type, name=name)\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[32mc:\\Users\\MendMa1\\AppData\\Local\\anaconda3\\envs\\colab\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, DstT, Truncate, name)\u001b[39m\n\u001b[32m   2334\u001b[39m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[32m   2335\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   2336\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2337\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m2338\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   2339\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2340\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2341\u001b[39m       return cast_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting extended quantum GAN training with comprehensive monitoring...\")\n",
    "print(\"This will take significantly longer but should show real learning!\")\n",
    "\n",
    "# Extended training\n",
    "training_history = trainer.train_with_monitoring(\n",
    "    data=real_data,\n",
    "    epochs=100,           # Much longer training\n",
    "    batch_size=8,         # Small batches for quantum stability\n",
    "    monitor_interval=5,   # Monitor every 5 epochs\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Extended training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1705a4c",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Training Analysis Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # or 'TkAgg' or 'Qt5Agg'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create comprehensive training dashboard\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "epochs = training_history['epochs']\n",
    "\n",
    "# 1. Loss Evolution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs, training_history['generator_losses'], label='Generator Loss', color='blue', linewidth=2)\n",
    "ax1.plot(epochs, training_history['discriminator_losses'], label='Discriminator Loss', color='red', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Evolution', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# 2. Quality Metrics Evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, training_history['mean_differences'], label='Mean Difference', color='green', linewidth=2)\n",
    "ax2.plot(epochs, training_history['std_differences'], label='Std Difference', color='orange', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Difference')\n",
    "ax2.set_title('Quality Metrics Evolution', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# 3. Wasserstein Distance\n",
    "ax3 = axes[1, 0]\n",
    "valid_wd = [wd for wd in training_history['wasserstein_distances'] if wd != float('inf')]\n",
    "valid_epochs = epochs[:len(valid_wd)]\n",
    "if valid_wd:\n",
    "    ax3.plot(valid_epochs, valid_wd, label='Wasserstein Distance', color='purple', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Wasserstein Distance')\n",
    "    ax3.set_title('Distribution Distance Evolution', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "\n",
    "# 4. Stability Metric\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(epochs, training_history['stability_metrics'], label='G/D Loss Ratio', color='brown', linewidth=2)\n",
    "ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Perfect Balance')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Stability Ratio')\n",
    "ax4.set_title('Training Stability', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "# 5. Final Generated vs Real Data\n",
    "ax5 = axes[2, 0]\n",
    "final_quality = trainer.compute_quality_metrics(real_data, n_samples=300)\n",
    "generated_samples = final_quality['generated_samples']\n",
    "\n",
    "ax5.scatter(real_data[:, 0], real_data[:, 1], alpha=0.6, s=20, color='blue', label='Real Data')\n",
    "ax5.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.6, s=20, color='red', label='Generated Data')\n",
    "ax5.set_xlabel('X₁')\n",
    "ax5.set_ylabel('X₂')\n",
    "ax5.set_title('Final: Real vs Generated Data', fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.axis('equal')\n",
    "\n",
    "# 6. Training Time Analysis\n",
    "ax6 = axes[2, 1]\n",
    "ax6.plot(epochs, training_history['training_times'], label='Time per Monitoring Interval', color='gray', linewidth=2)\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Time (seconds)')\n",
    "ax6.set_title('Training Time Analysis', fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.suptitle('Extended Quantum GAN Training Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print final analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTENDED TRAINING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "initial_mean_diff = training_history['mean_differences'][0]\n",
    "final_mean_diff = training_history['mean_differences'][-1]\n",
    "improvement = ((initial_mean_diff - final_mean_diff) / initial_mean_diff) * 100\n",
    "\n",
    "print(f\"\\nQuality Improvement:\")\n",
    "print(f\"  Initial Mean Difference: {initial_mean_diff:.4f}\")\n",
    "print(f\"  Final Mean Difference: {final_mean_diff:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Generator Loss: {training_history['generator_losses'][-1]:.4f}\")\n",
    "print(f\"  Discriminator Loss: {training_history['discriminator_losses'][-1]:.4f}\")\n",
    "print(f\"  Stability Ratio: {training_history['stability_metrics'][-1]:.4f}\")\n",
    "\n",
    "if valid_wd:\n",
    "    print(f\"  Final Wasserstein Distance: {valid_wd[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Total Epochs: {len(epochs)} monitoring points over {max(epochs)} epochs\")\n",
    "print(f\"  Architecture: Generator({len(generator.trainable_variables)} params), Discriminator({len(discriminator.trainable_variables)} params)\")\n",
    "print(f\"  Total Training Time: {sum(training_history['training_times']):.1f}s\")\n",
    "\n",
    "# Convergence analysis\n",
    "if len(training_history['mean_differences']) > 10:\n",
    "    recent_improvement = training_history['mean_differences'][-5:]\n",
    "    if max(recent_improvement) - min(recent_improvement) < 0.1:\n",
    "        print(f\"\\n✓ Training appears to have converged (stable quality in last 5 measurements)\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Training may benefit from additional epochs (quality still changing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d0bcb",
   "metadata": {},
   "source": [
    "## 6. Detailed Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive final evaluation\n",
    "print(\"Performing detailed quality assessment...\")\n",
    "\n",
    "# Generate larger sample for final evaluation\n",
    "final_evaluation = trainer.compute_quality_metrics(real_data, n_samples=500)\n",
    "\n",
    "print(f\"\\nFinal Quality Assessment (500 samples):\")\n",
    "print(f\"  Mean Difference: {final_evaluation['mean_difference']:.4f}\")\n",
    "print(f\"  Std Difference: {final_evaluation['std_difference']:.4f}\")\n",
    "print(f\"  Wasserstein Distance: {final_evaluation['wasserstein_distance']:.4f}\")\n",
    "\n",
    "# Quality benchmarks\n",
    "print(f\"\\nQuality Benchmarks:\")\n",
    "if final_evaluation['mean_difference'] < 0.5:\n",
    "    print(f\"  ✓ Excellent mean matching (< 0.5)\")\n",
    "elif final_evaluation['mean_difference'] < 1.0:\n",
    "    print(f\"  ✓ Good mean matching (< 1.0)\")\n",
    "elif final_evaluation['mean_difference'] < 2.0:\n",
    "    print(f\"  ⚠ Fair mean matching (< 2.0)\")\n",
    "else:\n",
    "    print(f\"  ✗ Poor mean matching (≥ 2.0)\")\n",
    "\n",
    "if final_evaluation['std_difference'] < 0.3:\n",
    "    print(f\"  ✓ Excellent variance matching (< 0.3)\")\n",
    "elif final_evaluation['std_difference'] < 0.6:\n",
    "    print(f\"  ✓ Good variance matching (< 0.6)\")\n",
    "else:\n",
    "    print(f\"  ⚠ Poor variance matching (≥ 0.6)\")\n",
    "\n",
    "# Distribution visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "generated_samples = final_evaluation['generated_samples']\n",
    "\n",
    "# Scatter plot comparison\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(real_data[:, 0], real_data[:, 1], alpha=0.6, s=15, color='blue', label='Real Data')\n",
    "ax1.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.6, s=15, color='red', label='Generated Data')\n",
    "ax1.set_xlabel('X₁')\n",
    "ax1.set_ylabel('X₂')\n",
    "ax1.set_title('Final Distribution Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axis('equal')\n",
    "\n",
    "# X1 marginal distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(real_data[:, 0].numpy(), bins=30, alpha=0.7, density=True, color='blue', label='Real X₁', histtype='step', linewidth=2)\n",
    "ax2.hist(generated_samples[:, 0], bins=30, alpha=0.7, density=True, color='red', label='Generated X₁', histtype='step', linewidth=2)\n",
    "ax2.set_xlabel('X₁ Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('X₁ Marginal Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# X2 marginal distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(real_data[:, 1].numpy(), bins=30, alpha=0.7, density=True, color='blue', label='Real X₂', histtype='step', linewidth=2)\n",
    "ax3.hist(generated_samples[:, 1], bins=30, alpha=0.7, density=True, color='red', label='Generated X₂', histtype='step', linewidth=2)\n",
    "ax3.set_xlabel('X₂ Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('X₂ Marginal Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Final Quality Assessment', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTENDED QUANTUM GAN TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(\"This notebook demonstrates proper quantum GAN training\")\n",
    "print(\"with comprehensive monitoring and quality assessment.\")\n",
    "print(\"Use this as a template for training more complex quantum GANs!\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
