# Data Directory

This directory is intended for storing datasets used in quantum GAN experiments.

## Dataset Requirements

The quantum GAN framework is designed to work with various types of datasets:

### Supported Data Formats

- **NumPy arrays** (`.npy` files)
- **CSV files** with numerical features
- **Synthetic datasets** generated by the framework

### Data Structure

Datasets should be structured as:
- **Shape**: `(n_samples, n_features)`
- **Type**: Floating-point values (preferably normalized)
- **Format**: Dense feature matrices

### Usage

The framework provides flexible data loading through `src/utils/data_utils.py`:

```python
from src.utils.data_utils import load_dataset, load_synthetic_data

# Load custom dataset
data = load_dataset(data_path="data", num_samples=1000, feature_dim=30)

# Generate synthetic data for testing
synthetic_data = load_synthetic_data(dataset_type="spiral", num_samples=1000)
```

### Synthetic Data Options

For testing and development, the framework can generate synthetic datasets:

- **spiral**: Spiral pattern in 2D
- **moons**: Two interleaving half circles
- **circles**: Two concentric circles  
- **gaussian**: Multi-modal Gaussian mixture

### Data Preprocessing

All datasets are automatically:
- Normalized using standard scaling (zero mean, unit variance)
- Converted to TensorFlow tensors when available
- Validated for proper shape and type

### Adding Custom Datasets

To use your own dataset:

1. Place data files in this directory
2. Ensure proper format (numerical features only)
3. Use the data loading utilities in `src/utils/data_utils.py`
4. Verify data shape matches your model requirements

### Example Directory Structure

```
data/
├── README.md                 # This file
├── features.npy             # Main dataset file
├── custom_dataset.csv       # Alternative format
└── experiments/             # Experimental datasets
    ├── experiment_1.npy
    └── experiment_2.npy
```

## Notes

- The framework does not include any specific dataset by default
- Users must provide their own datasets or use synthetic data generation
- All data loading includes proper error handling and fallback options
- Dataset size recommendations: minimum 1000 samples for meaningful results
